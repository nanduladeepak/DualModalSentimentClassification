{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"Dataset/Actor_01 2/01-01-05-02-02-01-01.mp4\"\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(path)\n\u001b[1;32m     29\u001b[0m ret, prev_frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[0;32m---> 30\u001b[0m prev_frame \u001b[39m=\u001b[39m detect_and_crop_face(prev_frame, face_cascade)\n\u001b[1;32m     31\u001b[0m prev_frame \u001b[39m=\u001b[39m  cv2\u001b[39m.\u001b[39mresize(prev_frame[\u001b[39m0\u001b[39m], (\u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m), interpolation\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mINTER_CUBIC)\n\u001b[1;32m     32\u001b[0m frame_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m, in \u001b[0;36mdetect_and_crop_face\u001b[0;34m(frame, face_cascade)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect_and_crop_face\u001b[39m(frame, face_cascade):\n\u001b[0;32m---> 14\u001b[0m     gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(frame, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)\n\u001b[1;32m     15\u001b[0m     faces \u001b[39m=\u001b[39m face_cascade\u001b[39m.\u001b[39mdetectMultiScale(\n\u001b[1;32m     16\u001b[0m         gray, scaleFactor\u001b[39m=\u001b[39m\u001b[39m1.1\u001b[39m, minNeighbors\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, minSize\u001b[39m=\u001b[39m(\u001b[39m30\u001b[39m, \u001b[39m30\u001b[39m))\n\u001b[1;32m     17\u001b[0m     cropped_faces \u001b[39m=\u001b[39m [gray[y:y\u001b[39m+\u001b[39mh, x:x\u001b[39m+\u001b[39mw] \u001b[39mfor\u001b[39;00m (x, y, w, h) \u001b[39min\u001b[39;00m faces]\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def mse(imageA, imageB):\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_and_crop_face(frame, face_cascade):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    cropped_faces = [gray[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
    "    return cropped_faces\n",
    "path = \"Dataset/Actor_01 2/01-01-05-02-02-01-01.mp4\"\n",
    "threshold = 400\n",
    "\n",
    "# Create a directory to store frames if it doesn't exist\n",
    "frames_dir = \"frames\"\n",
    "if not os.path.exists(frames_dir):\n",
    "    os.makedirs(frames_dir)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(path)\n",
    "ret, prev_frame = cap.read()\n",
    "prev_frame = detect_and_crop_face(prev_frame, face_cascade)\n",
    "prev_frame =  cv2.resize(prev_frame[0], (128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "frame_count = 0\n",
    "a_frame_count = 0\n",
    "\n",
    "while True:\n",
    "    # Read frame from video\n",
    "    ret, current_frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cropped_frame = detect_and_crop_face(current_frame, face_cascade)\n",
    "    \n",
    "    current_frame = cv2.resize(\n",
    "        cropped_frame[0], (128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Compute the MSE between the current frame and the previous frame\n",
    "    error = mse(prev_frame, current_frame)\n",
    "    print(error)\n",
    "    print(error)\n",
    "    if error > threshold:\n",
    "        # Save each frame as an image\n",
    "        frame_name = os.path.join(f\"{frames_dir}/accepted\", f\"frame_{frame_count}_{a_frame_count}.jpg\")\n",
    "        cv2.imwrite(frame_name, current_frame)\n",
    "        a_frame_count+=1\n",
    "    else :\n",
    "        # Save each frame as an image\n",
    "        frame_name = os.path.join(f\"{frames_dir}/reject\", f\"frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(frame_name, current_frame)\n",
    "    \n",
    "    # Save each frame as an image\n",
    "    frame_name = os.path.join(f\"{frames_dir}/all\", f\"frame_{frame_count}.jpg\")\n",
    "    cv2.imwrite(frame_name, current_frame)\n",
    "    frame_count += 1\n",
    "\n",
    "    prev_frame = current_frame\n",
    "    \n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271.29425048828125\n",
      "171.1339111328125\n",
      "134.55718994140625\n",
      "538.3927612304688\n",
      "263.07757568359375\n",
      "121.89776611328125\n",
      "80.45892333984375\n",
      "255.84210205078125\n",
      "471.97698974609375\n",
      "451.1783447265625\n",
      "197.2843017578125\n",
      "116.3748779296875\n",
      "495.9256591796875\n",
      "115.5882568359375\n",
      "262.21600341796875\n",
      "364.45428466796875\n",
      "73.36199951171875\n",
      "1461.9351196289062\n",
      "865.1307373046875\n",
      "332.69134521484375\n",
      "248.0526123046875\n",
      "225.779541015625\n",
      "381.355224609375\n",
      "200.1968994140625\n",
      "555.3580322265625\n",
      "772.255615234375\n",
      "859.9434204101562\n",
      "330.8348388671875\n",
      "555.7442016601562\n",
      "497.2330322265625\n",
      "659.1669311523438\n",
      "241.65576171875\n",
      "913.6115112304688\n",
      "235.91815185546875\n",
      "273.8702392578125\n",
      "550.7634887695312\n",
      "1163.7271728515625\n",
      "401.61944580078125\n",
      "269.302734375\n",
      "118.8228759765625\n",
      "419.39691162109375\n",
      "296.25799560546875\n",
      "441.6068115234375\n",
      "543.8380126953125\n",
      "735.5692138671875\n",
      "482.15533447265625\n",
      "351.83624267578125\n",
      "687.9968872070312\n",
      "570.915283203125\n",
      "275.121826171875\n",
      "469.57562255859375\n",
      "459.99176025390625\n",
      "545.1076049804688\n",
      "906.5824584960938\n",
      "326.43499755859375\n",
      "209.886474609375\n",
      "222.6885986328125\n",
      "176.871826171875\n",
      "261.36309814453125\n",
      "215.346923828125\n",
      "343.3912353515625\n",
      "404.20648193359375\n",
      "692.2921142578125\n",
      "222.8162841796875\n",
      "969.8953247070312\n",
      "697.575439453125\n",
      "109.93341064453125\n",
      "184.15869140625\n",
      "902.9039916992188\n",
      "195.1829833984375\n",
      "490.3294677734375\n",
      "740.6255493164062\n",
      "129.52716064453125\n",
      "634.2406616210938\n",
      "568.2843017578125\n",
      "362.8040771484375\n",
      "208.21954345703125\n",
      "351.35821533203125\n",
      "176.68829345703125\n",
      "922.6351928710938\n",
      "517.559326171875\n",
      "823.2091674804688\n",
      "310.34552001953125\n",
      "497.7225341796875\n",
      "196.5184326171875\n",
      "382.697265625\n",
      "164.1307373046875\n",
      "1130.7993774414062\n",
      "249.44329833984375\n",
      "714.3751831054688\n",
      "183.2618408203125\n",
      "732.36767578125\n",
      "470.96044921875\n",
      "525.629638671875\n",
      "167.276611328125\n",
      "438.345458984375\n",
      "62.2618408203125\n",
      "295.4715576171875\n",
      "575.822265625\n",
      "142.2357177734375\n",
      "479.02239990234375\n",
      "163.06396484375\n",
      "32.54791259765625\n",
      "315.79608154296875\n",
      "347.50909423828125\n",
      "276.91497802734375\n",
      "349.25592041015625\n",
      "538.803466796875\n",
      "424.7161865234375\n",
      "125.71337890625\n",
      "688.7587280273438\n",
      "534.7850952148438\n",
      "280.2130126953125\n",
      "324.41558837890625\n",
      "135.4248046875\n",
      "684.53759765625\n",
      "520.6683959960938\n",
      "439.73046875\n",
      "126.906005859375\n",
      "718.5504760742188\n",
      "135.30926513671875\n",
      "300.04290771484375\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def mse(imageA, imageB):\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_and_crop_face(frame, face_cascade):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    cropped_faces = [gray[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
    "    return cropped_faces\n",
    "\n",
    "path = \"Dataset/Actor_01/01-01-05-02-02-01-01.mp4\"\n",
    "threshold = 400\n",
    "\n",
    "# Create a directory to store frames if it doesn't exist\n",
    "frames_dir = \"frames\"\n",
    "if not os.path.exists(frames_dir):\n",
    "    os.makedirs(frames_dir)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise ValueError(f\"Could not open video file at path: {path}\")\n",
    "\n",
    "ret, prev_frame = cap.read()\n",
    "\n",
    "if not ret or prev_frame is None:\n",
    "    raise ValueError(\"Error reading the first frame or the frame is empty\")\n",
    "\n",
    "cropped_faces = detect_and_crop_face(prev_frame, face_cascade)\n",
    "if len(cropped_faces) == 0:\n",
    "    raise ValueError(\"Could not detect a face in the first frame\")\n",
    "\n",
    "prev_frame = cv2.resize(cropped_faces[0], (48, 48), interpolation=cv2.INTER_CUBIC)\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, current_frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cropped_faces = detect_and_crop_face(current_frame, face_cascade)\n",
    "    if len(cropped_faces) == 0:\n",
    "        # Skip this frame if no face is detected\n",
    "        continue\n",
    "\n",
    "    current_frame = cv2.resize(cropped_faces[0], (48, 48), interpolation=cv2.INTER_CUBIC)\n",
    "    error = mse(prev_frame, current_frame)\n",
    "    print(error)\n",
    "\n",
    "    # if error > threshold:\n",
    "    frame_name = os.path.join(frames_dir, f\"frame_{frame_count}.jpg\")\n",
    "    cv2.imwrite(frame_name, current_frame)\n",
    "    frame_count += 1\n",
    "    prev_frame = current_frame\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Custom Attention Layer\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")        \n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        et = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        at = tf.keras.backend.softmax(et)\n",
    "        at = tf.keras.backend.repeat_elements(at, x.shape[-1], axis=2)\n",
    "        output = x * at\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "# CNNs for Audio and Video\n",
    "audio_input = tf.keras.layers.Input(shape=(162,1))\n",
    "audio_model = tf.keras.layers.Conv1D(128, kernel_size=5, strides=1, padding='same',  activation='relu')(audio_input)\n",
    "audio_model = tf.keras.layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(audio_model)\n",
    "audio_model = tf.keras.layers.Conv1D(64, kernel_size=5, strides=1, padding='same',  activation='relu')(audio_input)\n",
    "audio_model = tf.keras.layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(audio_model)\n",
    "audio_model = tf.keras.layers.Dropout(0.2)(audio_model)\n",
    "audio_model = tf.keras.layers.Conv1D(32, kernel_size=5, strides=1, padding='same',  activation='relu')(audio_input)\n",
    "audio_model = tf.keras.layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(audio_model)\n",
    "audio_model = tf.keras.layers.Flatten()(audio_model)\n",
    "\n",
    "video_input = tf.keras.layers.Input(shape=(100,128,128))\n",
    "video_model = tf.keras.layers.Conv2D(128, (3, 3), activation='relu')(video_input)\n",
    "video_model = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(video_model)\n",
    "video_model = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(video_input)\n",
    "video_model = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(video_model)\n",
    "video_model = tf.keras.layers.Dropout(0.2)(video_model)\n",
    "video_model = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(video_input)\n",
    "video_model = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(video_model)\n",
    "video_model = tf.keras.layers.Flatten()(video_model)\n",
    "\n",
    "combined = tf.keras.layers.concatenate([audio_model, video_model])\n",
    "\n",
    "# MLP for Classification\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(combined)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "output = tf.keras.layers.Dense(8, activation='softmax')(x)\n",
    "\n",
    "# Compile and Train\n",
    "model = tf.keras.models.Model(inputs=[audio_input, video_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to open: Dataset/Actor_01/01-01-01-01-01-01-01.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"Dataset/Actor_01/01-01-01-02-01-01-01.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"Dataset/Actor_01/01-01-01-02-01-02-01.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"Dataset/Actor_01/01-01-01-02-02-01-01.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"Dataset/Actor_01/01-01-01-02-02-02-01.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to open: Dataset/Actor_01/01-01-01-02-01-01-01.mp4\n",
      "failed to open: Dataset/Actor_01/01-01-01-02-01-02-01.mp4\n",
      "failed to open: Dataset/Actor_01/01-01-01-02-02-01-01.mp4\n",
      "failed to open: Dataset/Actor_01/01-01-01-02-02-02-01.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "\n",
    "\n",
    "def detect_and_crop_face(frame, face_cascade):\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    cropped_faces = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        cropped_face = gray[y:y+h, x:x+w]\n",
    "        cropped_faces.append(cropped_face)\n",
    "    \n",
    "    return cropped_faces\n",
    "\n",
    "# Load the Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "\n",
    "def mse(imageA, imageB):\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err\n",
    "\n",
    "actorsId =  [\"{:02}\".format(i) for i in range(1, 25)]\n",
    "comboFiles=['01-01-01', '01-01-02', '01-02-01', '01-02-02', '02-01-01', '02-01-02', '02-02-01', '02-02-02']\n",
    "\n",
    "for actor in actorsId:\n",
    "    emotion = 1\n",
    "    while emotion<9:\n",
    "        for file in comboFiles:\n",
    "            try:\n",
    "                path = f\"Dataset/Actor_{actor}/01-01-0{emotion}-{file}-{actor}.mp4\"\n",
    "                threshold = 400\n",
    "\n",
    "                # Create a directory to store frames if it doesn't exist\n",
    "                frames_dir = f\"frames/accepted/{actor}\"\n",
    "                if not os.path.exists(frames_dir):\n",
    "                    os.makedirs(frames_dir)\n",
    "\n",
    "                # Open the video file\n",
    "                cap = cv2.VideoCapture(path)\n",
    "                clip = VideoFileClip(path)\n",
    "                audio = clip.audio\n",
    "                ret, prev_frame = cap.read()\n",
    "                frame_count = 0\n",
    "                audio_path = f\"{frames_dir}/01-01-0{emotion}-{file}-{actor}.wav\"\n",
    "                audio.write_audiofile(audio_path)\n",
    "                while True:\n",
    "                    # Read frame from video\n",
    "                    ret, current_frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    # Compute the MSE between the current frame and the previous frame\n",
    "                    error = mse(prev_frame, current_frame)\n",
    "                    # print(error)\n",
    "                    if error > threshold:\n",
    "                        # Save each frame as an image\n",
    "                        frame_name = os.path.join(frames_dir, f\"frame_01-01-0{emotion}-{file}-{actor}_{frame_count}.jpg\")\n",
    "                        cropped_frame = detect_and_crop_face(current_frame, face_cascade)\n",
    "                        cv2.imwrite(frame_name, cropped_frame[0])\n",
    "                        frame_count += 1\n",
    "                        prev_frame = current_frame\n",
    "            except:\n",
    "                print(f\"failed to open: Dataset/Actor_{actor}/01-01-0{emotion}-{file}-{actor}.mp4\")\n",
    "        emotion+=1\n",
    "    \n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "video = VideoFileClip(path)\n",
    "audio = video.audio\n",
    "# audio_path = \"audio.wav\"\n",
    "# audio.write_audiofile(audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def detect_and_crop_face(frame, face_cascade):\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    cropped_faces = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        cropped_face = frame[y:y+h, x:x+w]\n",
    "        cropped_faces.append(cropped_face)\n",
    "    \n",
    "    return cropped_faces\n",
    "\n",
    "# Load the Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Assuming video_frames is a list of frames from your video\n",
    "video_frames = ...  # your list of frames\n",
    "\n",
    "all_cropped_faces = []\n",
    "\n",
    "for frame in video_frames:\n",
    "    cropped_faces = detect_and_crop_face(frame, face_cascade)\n",
    "    all_cropped_faces.extend(cropped_faces)\n",
    "\n",
    "# Now, all_cropped_faces contains all the cropped face images from all the frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Dataset/Actor_01/01-01-01-02-01-01-01.mp4: MoviePy error: the file Dataset/Actor_01/01-01-01-02-01-01-01.mp4 could not be found!\n",
      "Please check that you entered the correct path.\n",
      "Error processing Dataset/Actor_01/01-01-01-02-01-02-01.mp4: MoviePy error: the file Dataset/Actor_01/01-01-01-02-01-02-01.mp4 could not be found!\n",
      "Please check that you entered the correct path.\n",
      "Error processing Dataset/Actor_01/01-01-01-02-02-01-01.mp4: MoviePy error: the file Dataset/Actor_01/01-01-01-02-02-01-01.mp4 could not be found!\n",
      "Please check that you entered the correct path.\n",
      "Error processing Dataset/Actor_01/01-01-01-02-02-02-01.mp4: MoviePy error: the file Dataset/Actor_01/01-01-01-02-02-02-01.mp4 could not be found!\n",
      "Please check that you entered the correct path.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     prev_frame \u001b[39m=\u001b[39m current_frame\n\u001b[1;32m     42\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m error \u001b[39m=\u001b[39m mse(prev_frame, current_frame)\n\u001b[1;32m     45\u001b[0m \u001b[39mif\u001b[39;00m error \u001b[39m>\u001b[39m threshold:\n\u001b[1;32m     46\u001b[0m     frame_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(frames_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mframe_01-01-0\u001b[39m\u001b[39m{\u001b[39;00memotion\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mactor\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mframe_count\u001b[39m}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m, in \u001b[0;36mmse\u001b[0;34m(imageA, imageB)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmse\u001b[39m(imageA, imageB):\n\u001b[0;32m---> 13\u001b[0m     err \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum((imageA\u001b[39m.\u001b[39;49mastype(\u001b[39m\"\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m-\u001b[39;49m imageB\u001b[39m.\u001b[39;49mastype(\u001b[39m\"\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m     14\u001b[0m     err \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(imageA\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m imageA\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m err\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "\n",
    "def detect_and_crop_face(frame, face_cascade):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    cropped_faces = [gray[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
    "    return cropped_faces\n",
    "\n",
    "def mse(imageA, imageB):\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "actorsId =  [\"{:02}\".format(i) for i in range(1, 25)]\n",
    "comboFiles=['01-01-01', '01-01-02', '01-02-01', '01-02-02', '02-01-01', '02-01-02', '02-02-01', '02-02-02']\n",
    "\n",
    "threshold = 400\n",
    "\n",
    "for actor in actorsId:\n",
    "    for emotion in range(1, 9):\n",
    "        for file in comboFiles:\n",
    "            try:\n",
    "                path = f\"Dataset/Actor_{actor}/01-01-0{emotion}-{file}-{actor}.mp4\"\n",
    "                frames_dir = f\"frames/accepted/{actor}\"\n",
    "                if not os.path.exists(frames_dir):\n",
    "                    os.makedirs(frames_dir)\n",
    "\n",
    "                clip = VideoFileClip(path)\n",
    "                audio_path = f\"{frames_dir}/01-01-0{emotion}-{file}-{actor}.wav\"\n",
    "                # clip.audio.write_audiofile(audio_path)\n",
    "\n",
    "                prev_frame = None\n",
    "                frame_count = 0\n",
    "                \n",
    "                for current_frame in clip.iter_frames(fps=clip.fps, dtype='uint8'):\n",
    "                    if prev_frame is None:\n",
    "                        prev_frame = current_frame\n",
    "                        continue\n",
    "\n",
    "                    error = mse(prev_frame, current_frame)\n",
    "                    if error > threshold:\n",
    "                        frame_name = os.path.join(frames_dir, f\"frame_01-01-0{emotion}-{file}-{actor}_{frame_count}.jpg\")\n",
    "                        cropped_frame = detect_and_crop_face(current_frame, face_cascade)\n",
    "                        if cropped_frame:\n",
    "                            compressedFrame = cv2.resize(cropped_frame[0], (128,128), interpolation=cv2.INTER_CUBIC)\n",
    "                            cv2.imwrite(frame_name, compressedFrame)\n",
    "                            frame_count += 1\n",
    "                            prev_frame = current_frame\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "\n",
    "def detect_and_crop_face(frame, face_cascade):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    cropped_faces = [gray[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
    "    return cropped_faces\n",
    "\n",
    "def mse(imageA, imageB):\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "actorsId =  [\"{:02}\".format(i) for i in range(1, 25)]\n",
    "comboFiles=['01-01-01', '01-01-02', '01-02-01', '01-02-02', '02-01-01', '02-01-02', '02-02-01', '02-02-02']\n",
    "\n",
    "threshold = 400\n",
    "\n",
    "for actor in actorsId:\n",
    "    for emotion in range(1, 9):\n",
    "        for file in comboFiles:\n",
    "            try:\n",
    "                path = f\"Dataset/Actor_{actor}/01-01-0{emotion}-{file}-{actor}.mp4\"\n",
    "                frames_dir = f\"frames/accepted/{actor}\"\n",
    "                if not os.path.exists(frames_dir):\n",
    "                    os.makedirs(frames_dir)\n",
    "\n",
    "                clip = VideoFileClip(path)\n",
    "                audio_path = f\"{frames_dir}/01-01-0{emotion}-{file}-{actor}.wav\"\n",
    "                clip.audio.write_audiofile(audio_path)\n",
    "\n",
    "                prev_frame = None\n",
    "                frame_count = 0\n",
    "                \n",
    "                for current_frame in clip.iter_frames(fps=clip.fps, dtype='uint8'):\n",
    "                    if prev_frame is None:\n",
    "                        prev_frame = current_frame\n",
    "                        continue\n",
    "\n",
    "                    error = mse(prev_frame, current_frame)\n",
    "                    if error > threshold:\n",
    "                        frame_name = os.path.join(frames_dir, f\"frame_01-01-0{emotion}-{file}-{actor}_{frame_count}.jpg\")\n",
    "                        cropped_frame = detect_and_crop_face(current_frame, face_cascade)\n",
    "                        if cropped_frame:\n",
    "                            compressedFrame = cv2.resize(cropped_frame[0], (128,128), interpolation=cv2.INTER_CUBIC)\n",
    "                            cv2.imwrite(frame_name, compressedFrame)\n",
    "                            frame_count += 1\n",
    "                            prev_frame = current_frame\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Conv2D, MaxPooling2D, Flatten, concatenate, Attention\n",
    "\n",
    "# Audio CNN\n",
    "audio_input = Input(shape=(your_audio_shape))\n",
    "audio_model = Conv2D(32, (3, 3), activation='relu')(audio_input)\n",
    "audio_model = MaxPooling2D(pool_size=(2, 2))(audio_model)\n",
    "audio_model = Flatten()(audio_model)\n",
    "\n",
    "# Video CNN\n",
    "video_input = Input(shape=(your_video_shape))\n",
    "video_model = Conv2D(32, (3, 3), activation='relu')(video_input)\n",
    "video_model = MaxPooling2D(pool_size=(2, 2))(video_model)\n",
    "video_model = Flatten()(video_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = concatenate([audio_model, video_model])\n",
    "\n",
    "# Assuming each sequence (video + audio combined) has 'num_steps' steps\n",
    "reshaped = tf.reshape(combined, (-1, num_steps, combined.shape[1]))\n",
    "\n",
    "# LSTM\n",
    "lstm_out, lstm_state_h, lstm_state_c = LSTM(128, return_sequences=True, return_state=True)(reshaped)\n",
    "\n",
    "# Attention\n",
    "query_value_attention_seq = Attention()([lstm_out, lstm_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(64, activation='relu')(query_value_attention_seq)\n",
    "x = Flatten()(x)  # If you still have sequences to flatten after attention\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[audio_input, video_input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([audio_data, video_data], labels, epochs=your_epochs, batch_size=your_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")        \n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        et = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        at = tf.keras.backend.softmax(et)\n",
    "        at = tf.keras.backend.repeat_elements(at, x.shape[-1], axis=2)\n",
    "        output = x * at\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "# After LSTM\n",
    "lstm_out, _, _ = LSTM(128, return_sequences=True, return_state=True)(reshaped)\n",
    "\n",
    "# Add the custom attention layer\n",
    "attention_output = AttentionLayer()(lstm_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Conv2D, MaxPooling2D, Flatten, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Custom Attention Layer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")        \n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        et = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        at = tf.keras.backend.softmax(et)\n",
    "        at = tf.keras.backend.repeat_elements(at, x.shape[-1], axis=2)\n",
    "        output = x * at\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "# CNNs for Audio and Video\n",
    "audio_input = Input(shape=(your_audio_shape))\n",
    "audio_model = Conv2D(32, (3, 3), activation='relu')(audio_input)\n",
    "audio_model = MaxPooling2D(pool_size=(2, 2))(audio_model)\n",
    "audio_model = Flatten()(audio_model)\n",
    "\n",
    "video_input = Input(shape=(your_video_shape))\n",
    "video_model = Conv2D(32, (3, 3), activation='relu')(video_input)\n",
    "video_model = MaxPooling2D(pool_size=(2, 2))(video_model)\n",
    "video_model = Flatten()(video_model)\n",
    "\n",
    "# RNN with LSTM and Attention\n",
    "combined = concatenate([audio_model, video_model])\n",
    "reshaped = tf.reshape(combined, (-1, num_steps, combined.shape[1]))\n",
    "lstm_out, _, _ = LSTM(128, return_sequences=True, return_state=True)(reshaped)\n",
    "attention_output = AttentionLayer()(lstm_out)\n",
    "\n",
    "# MLP for Classification\n",
    "x = Dense(64, activation='relu')(attention_output)\n",
    "output = Dense(8, activation='softmax')(x)\n",
    "\n",
    "# Compile and Train\n",
    "model = Model(inputs=[audio_input, video_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # adjust the loss based on label format\n",
    "model.fit([audio_data, video_data], labels, epochs=your_epochs, batch_size=your_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class Denoise(Model):\n",
    "  def __init__(self):\n",
    "    super(Denoise, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Input(shape=(28, 28, 1)),\n",
    "      layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
    "      layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])\n",
    "\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "      layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "      layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = Denoise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(x_test_noisy).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    print(data)\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sample_rate = librosa.load('Dataset/Audio_Song_Actors_01-24/Actor_01/03-02-01-01-01-01-01.wav')\n",
    "data = librosa.to_mono(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    # ZCR\n",
    "    result = np.array([])\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result=np.hstack((result, zcr)) # stacking horizontally\n",
    "\n",
    "    # Chroma_stft\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
    "\n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms)) # stacking horizontally\n",
    "\n",
    "    # MelSpectogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mel)) # stacking horizontally\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_features(path):\n",
    "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
    "    data, sample_rate = librosa.load(path)\n",
    "    \n",
    "    # without augmentation\n",
    "    res1 = extract_features(data)\n",
    "    result = np.array(res1)\n",
    "    \n",
    "    # data with noise\n",
    "    noise_data = noise(data)\n",
    "    res2 = extract_features(noise_data)\n",
    "    result = np.vstack((result, res2)) # stacking vertically\n",
    "    \n",
    "    # # data with stretching and pitching\n",
    "    # new_data = stretch(data)\n",
    "    # data_stretch_pitch = pitch(new_data, sample_rate)\n",
    "    # res3 = extract_features(data_stretch_pitch)\n",
    "    # result = np.vstack((result, res3)) # stacking vertically\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_features('Dataset/Audio_Song_Actors_01-24/Actor_01/03-02-01-01-01-01-01.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 162)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93439,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['vid id', 'noise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(grouped_df, batch_size):\n",
    "    batches = []\n",
    "    labels = []\n",
    "    for name, group in grouped_df:\n",
    "        video_frames = group.sort_values(by=['frame count'])\n",
    "        for i in range(0, len(video_frames), batch_size):\n",
    "            batches.append(video_frames.iloc[i:i+batch_size])\n",
    "            labels.append(video_frames['emotion'].iloc[0])  # Assuming emotion is same for all frames of a video\n",
    "    return batches, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    max_length = max(batch['frame count']) + 1\n",
    "    padded_audio = tf.keras.preprocessing.sequence.pad_sequences(batch['audio features'], maxlen=max_length, padding='post', dtype='float32')\n",
    "    padded_video = tf.keras.preprocessing.sequence.pad_sequences(batch['frame'], maxlen=max_length, padding='post', dtype='float32')\n",
    "    return padded_audio, padded_video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batches, batch_labels = create_batches(grouped, batch_size)\n",
    "\n",
    "for idx, batch in enumerate(batches):\n",
    "    padded_audio, padded_video = pad_batch(batch)\n",
    "    labels = tf.keras.utils.to_categorical(batch_labels[idx], num_classes=8)\n",
    "    model.train_on_batch([padded_audio, padded_video], labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant\n",
    "MAX_FRAMES = 149\n",
    "\n",
    "def pad_sequences(df):\n",
    "    grouped = df.groupby(['vid id', 'noise'])\n",
    "\n",
    "    padded_audios = []\n",
    "    padded_videos = []\n",
    "    labels = []\n",
    "\n",
    "    for name, group in grouped:\n",
    "        group = group.sort_values(by=['frame count'])\n",
    "\n",
    "        audio_data = group['audio features'].tolist()\n",
    "        video_data = group['frame'].tolist()\n",
    "\n",
    "        # Pad sequences for audio and video data\n",
    "        audio_padding_amount = MAX_FRAMES - len(audio_data)\n",
    "        video_padding_amount = MAX_FRAMES - len(video_data)\n",
    "\n",
    "        audio_data.extend([np.zeros((162,)) for _ in range(audio_padding_amount)])\n",
    "        video_data.extend([np.zeros((128, 128)) for _ in range(video_padding_amount)])\n",
    "\n",
    "        padded_audios.append(np.array(audio_data))\n",
    "        padded_videos.append(np.array(video_data))\n",
    "        \n",
    "        labels.append(group['emotion'].iloc[0])\n",
    "\n",
    "    print(\"Shapes before final conversion:\")\n",
    "    print(np.shape(padded_audios))\n",
    "    print(np.shape(padded_videos))\n",
    "\n",
    "    return np.array(padded_audios), np.array(padded_videos), np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Custom Attention Layer\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")        \n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        et = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        at = tf.keras.backend.softmax(et)\n",
    "        at = tf.keras.backend.repeat_elements(at, x.shape[-1], axis=2)\n",
    "        output = x * at\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "# CNNs for Audio and Video\n",
    "audio_input = tf.keras.layers.Input(shape=(162,1))\n",
    "audio_model = tf.keras.layers.Conv1D(128, kernel_size=5, strides=1, padding='same',  activation='relu')(audio_input)\n",
    "audio_model = tf.keras.layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(audio_model)\n",
    "audio_model = tf.keras.layers.Conv1D(64, kernel_size=5, strides=1, padding='same',  activation='relu')(audio_input)\n",
    "audio_model = tf.keras.layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(audio_model)\n",
    "audio_model = tf.keras.layers.Dropout(0.2)(audio_model)\n",
    "audio_model = tf.keras.layers.Conv1D(32, kernel_size=5, strides=1, padding='same',  activation='relu')(audio_input)\n",
    "audio_model = tf.keras.layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(audio_model)\n",
    "audio_model = tf.keras.layers.Flatten()(audio_model)\n",
    "\n",
    "video_input = tf.keras.layers.Input(shape=(128,128,1))\n",
    "video_model = tf.keras.layers.Conv2D(128, (3, 3), activation='relu')(video_input)\n",
    "video_model = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(video_model)\n",
    "video_model = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(video_input)\n",
    "video_model = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(video_model)\n",
    "video_model = tf.keras.layers.Dropout(0.2)(audio_model)\n",
    "video_model = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(video_input)\n",
    "video_model = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(video_model)\n",
    "video_model = tf.keras.layers.Flatten()(video_model)\n",
    "\n",
    "# RNN with LSTM and Attention\n",
    "combined = tf.keras.layers.concatenate([audio_model, video_model])\n",
    "reshaped = tf.keras.layers.Reshape((1, combined.shape[1]))(combined)\n",
    "\n",
    "# Add a masking layer.\n",
    "masked = tf.keras.layers.Masking(mask_value=0.)(reshaped)\n",
    "\n",
    "lstm_out, _, _ = tf.keras.layers.LSTM(128, return_sequences=True, return_state=True)(masked)\n",
    "attention_output = AttentionLayer()(lstm_out)\n",
    "\n",
    "# MLP for Classification\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(attention_output)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "output = tf.keras.layers.Dense(8, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoder\n",
    "========================================================================================================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers, losses\n",
    "# from tensorflow.keras.datasets import fashion_mnist\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "class Denoise(tf.keras.models.Model):\n",
    "  def __init__(self):\n",
    "    super(Denoise, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      tf.keras.layers.Input(shape=(128, 128, 1)),\n",
    "      tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
    "      # tf.keras.layers.MaxPooling2D((2,2),padding = 'same'),\n",
    "      tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
    "      # tf.keras.layers.MaxPooling2D((2,2),padding = 'same')\n",
    "      ])\n",
    "\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "      tf.keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "      tf.keras.layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = Denoise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(),metrics='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataframe into 80% train and 20% temporary (test + validation)\n",
    "train_df, temp_df = train_test_split(df_filtered, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the temporary dataframe into equal parts for test and validation\n",
    "test_df, val_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImage = np.stack(train_df['frame'].to_numpy())\n",
    "valImage = np.stack(val_df['frame'].to_numpy())\n",
    "batch_size = trainImage.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(trainImage, trainImage,\n",
    "                epochs=4,\n",
    "                # batch_size=2869,\n",
    "                shuffle=True,\n",
    "                validation_data=(valImage, valImage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(x_test_noisy).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_dim_k, in_dim_q, out_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = out_dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q = tf.keras.layers.Dense(out_dim, use_bias=qkv_bias)\n",
    "        self.kv = tf.keras.layers.Dense(out_dim * 2, use_bias=qkv_bias)\n",
    "        self.attn_drop = tf.keras.layers.Dropout(attn_drop)\n",
    "        self.proj = tf.keras.layers.Dense(out_dim)\n",
    "        self.proj_drop = tf.keras.layers.Dropout(proj_drop)\n",
    "        self.qkmatrix = None\n",
    "\n",
    "    def call(self, x, x_q):\n",
    "        B, Nk, Ck = x.shape\n",
    "        B, Nq, Cq = x_q.shape\n",
    "        q = tf.reshape(self.q(x_q), [B, Nq, 1, self.num_heads, -1])\n",
    "        q = tf.transpose(q, perm=[2, 0, 3, 1, 4])\n",
    "        kv = tf.reshape(self.kv(x), [B, Nk, 2, self.num_heads, -1])\n",
    "        kv = tf.transpose(kv, perm=[2, 0, 3, 1, 4])\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        q = tf.squeeze(q, axis=0)\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        self.qkmatrix = attn\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = tf.reshape(tf.transpose(tf.matmul(attn, v), perm=[1, 2, 0, 3]), [B, Nq, -1])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, self.qkmatrix\n",
    "\n",
    "# Video stages\n",
    "def video_stages(input_shape):\n",
    "    video_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu')(video_input)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    shape = x.get_shape().as_list()\n",
    "    x = tf.keras.layers.Reshape((shape[1]*shape[2], shape[3]))(x)\n",
    "    x = tf.keras.layers.Conv1D(128, 3, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Conv1D(64, 3, activation='relu')(x)\n",
    "    \n",
    "    # Stage 2\n",
    "    x2 = tf.keras.layers.Conv1D(64, 3, activation='relu')(x)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(pool_size=2)(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(32, 3, activation='relu')(x2)\n",
    "    \n",
    "    return tf.keras.Model(inputs=video_input, outputs=x, name=\"video_stage1_model\"), \\\n",
    "           tf.keras.Model(inputs=video_input, outputs=x2, name=\"video_stage2_model\")\n",
    "\n",
    "# Audio stages\n",
    "def audio_stages(input_shape, input_shape_stage2=None):\n",
    "    if input_shape_stage2 is None:\n",
    "        input_shape_stage2 = input_shape\n",
    "    \n",
    "    audio_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(audio_input)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x)\n",
    "    \n",
    "    # Stage 2\n",
    "    audio_input_stage2 = tf.keras.layers.Input(shape=input_shape_stage2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu')(audio_input_stage2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(32, kernel_size=5, strides=1, padding='same', activation='relu')(x2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x2)\n",
    "    \n",
    "    return tf.keras.Model(inputs=audio_input, outputs=x, name=\"audio_stage1_model\"), \\\n",
    "           tf.keras.Model(inputs=audio_input_stage2, outputs=x2, name=\"audio_stage2_model\")\n",
    "\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, video_input_shape, audio_input_shape, num_heads=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.video_stage1, self.video_stage2 = video_stages(video_input_shape)\n",
    "        # self.audio_stage1, self.audio_stage2 = audio_stages(audio_input_shape)\n",
    "\n",
    "\n",
    "\n",
    "        # Getting the shapes after stage 1 transformations\n",
    "        video_output_shape = self.video_stage1.layers[-1].output_shape[1:]\n",
    "        audio_output_shape = self.audio_stage1.layers[-1].output_shape[1:]\n",
    "\n",
    "\n",
    "        audio_output_shape_after_attention = (audio_output_shape[0], 64) # modify as per your architecture\n",
    "\n",
    "        self.audio_stage1, self.audio_stage2 = audio_stages(audio_input_shape, audio_output_shape_after_attention)\n",
    "\n",
    "        self.av1 = Attention(in_dim_k=video_output_shape[-1], in_dim_q=audio_output_shape[-1], out_dim=audio_output_shape[-1], num_heads=num_heads)\n",
    "        self.va1 = Attention(in_dim_k=audio_output_shape[-1], in_dim_q=video_output_shape[-1], out_dim=video_output_shape[-1], num_heads=num_heads)\n",
    "\n",
    "        self.final_mlp_layer_1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.final_mlp_layer_output = tf.keras.layers.Dense(8, activation='softmax', name=\"final_classification\")\n",
    "\n",
    "    def call(self, x_audio, x_visual):\n",
    "        x_audio = self.audio_stage1(x_audio)\n",
    "        x_visual = self.video_stage1(x_visual)\n",
    "\n",
    "        proj_x_a = tf.transpose(x_audio, perm=[0,2,1])\n",
    "        proj_x_v = tf.transpose(x_visual, perm=[0,2,1])\n",
    "\n",
    "        _, h_av = self.av1(proj_x_v, proj_x_a)\n",
    "        _, h_va = self.va1(proj_x_a, proj_x_v)\n",
    "\n",
    "        if h_av.shape[1] > 1:  # if more than 1 head, take average\n",
    "            h_av = tf.reduce_mean(h_av, axis=1, keepdims=True)\n",
    "        h_av = tf.reduce_sum(h_av, axis=-2)\n",
    "\n",
    "        if h_va.shape[1] > 1:  # if more than 1 head, take average\n",
    "            h_va = tf.reduce_mean(h_va, axis=1, keepdims=True)\n",
    "        h_va = tf.reduce_sum(h_va, axis=-2)\n",
    "\n",
    "        x_audio = h_va * x_audio\n",
    "        x_visual = h_av * x_visual\n",
    "        \n",
    "        # Passing through stage 2\n",
    "        x_audio = self.audio_stage2(x_audio)\n",
    "        x_visual = self.video_stage2(x_visual)\n",
    "\n",
    "        combined = tf.keras.layers.concatenate([x_audio, x_visual])\n",
    "        hidden = self.final_mlp_layer_1(combined)\n",
    "        sentiment = self.final_mlp_layer_output(hidden)\n",
    "        \n",
    "        return sentiment\n",
    "\n",
    "model = Model(video_input_shape=(100, 128, 128), audio_input_shape=(162, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_dim_k, in_dim_q, out_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        head_dim = out_dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # Define the layers\n",
    "        self.q = tf.keras.layers.Dense(out_dim, use_bias=qkv_bias)\n",
    "        self.kv = tf.keras.layers.Dense(out_dim * 2, use_bias=qkv_bias)\n",
    "        self.attn_drop = tf.keras.layers.Dropout(attn_drop)\n",
    "        self.proj = tf.keras.layers.Dense(out_dim)\n",
    "        self.proj_drop = tf.keras.layers.Dropout(proj_drop)\n",
    "        self.qkmatrix = None\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, x_q = inputs\n",
    "        _, Nk, Ck = x.shape\n",
    "        _, Nq, Cq = x_q.shape\n",
    "\n",
    "        q = self.q(x_q)\n",
    "        q = tf.reshape(q, [-1, Nq, self.num_heads, tf.shape(q)[-1]])\n",
    "        q = tf.transpose(q, [0, 2, 1, 3])\n",
    "\n",
    "        kv = self.kv(x)\n",
    "        kv = tf.reshape(kv, [-1, Nk, 2, self.num_heads, tf.shape(kv)[-1] // 2])\n",
    "        kv = tf.transpose(kv, [2, 0, 3, 1, 4])\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        \n",
    "        self.qkmatrix = attn\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = tf.matmul(attn, v)\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        x = tf.reshape(x, [-1, Nq, self.num_heads * tf.shape(x)[-1]])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x, self.qkmatrix\n",
    "\n",
    "\n",
    "# Video stages\n",
    "def video_stage1(input_shape):\n",
    "    video_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu')(video_input)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    shape = x.get_shape().as_list()\n",
    "    x = tf.keras.layers.Reshape((shape[1]*shape[2], shape[3]))(x)\n",
    "    x = tf.keras.layers.Conv1D(128, 3, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Conv1D(64, 3, activation='relu')(x)\n",
    "    return tf.keras.Model(inputs=video_input, outputs=x, name=\"video_stage1_model\")\n",
    "\n",
    "def video_stage2(input_shape_stage2):\n",
    "    # Stage 2\n",
    "    vedio_input_stage2 = tf.keras.layers.Input(shape=input_shape_stage2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 3, activation='relu')(vedio_input_stage2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(pool_size=2)(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(32, 3, activation='relu')(x2)\n",
    "    return tf.keras.Model(inputs=vedio_input_stage2, outputs=x2, name=\"video_stage2_model\")\n",
    "\n",
    "def audio_stage1(input_shape):\n",
    "    audio_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    # x = tf.keras.layers.Reshape((-1, 1))(audio_input)\n",
    "    x = tf.keras.layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(audio_input)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x)\n",
    "    return tf.keras.Model(inputs=audio_input, outputs=x, name=\"audio_stage1_model\")\n",
    "\n",
    " \n",
    "def audio_stage2(input_shape_stage2):\n",
    "    # Stage 2\n",
    "    audio_input_stage2 = tf.keras.layers.Input(shape=input_shape_stage2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu')(audio_input_stage2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(32, kernel_size=5, strides=1, padding='same', activation='relu')(x2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x2)\n",
    "    \n",
    "    return tf.keras.Model(inputs=audio_input_stage2, outputs=x2, name=\"audio_stage2_model\")\n",
    "\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, video_input_shape, audio_input_shape, num_heads=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.video_stage1 = video_stage1(video_input_shape)\n",
    "        self.audio_stage1 = audio_stage1(audio_input_shape)\n",
    "\n",
    "\n",
    "\n",
    "        # Getting the shapes after stage 1 transformations\n",
    "        video_output_shape = self.video_stage1.layers[-1].output_shape[1:]\n",
    "        audio_output_shape = self.audio_stage1.layers[-1].output_shape[1:]\n",
    "\n",
    "\n",
    "        audio_output_shape_after_attention = (audio_output_shape[0], 64)\n",
    "        vedio_output_shape_after_attention = (video_output_shape[0], 64)\n",
    "\n",
    "        self.audio_stage2 = audio_stage2(audio_output_shape_after_attention)\n",
    "        self.video_stage2 = video_stage2(vedio_output_shape_after_attention)\n",
    "\n",
    "        self.av1 = Attention(in_dim_k=video_output_shape[-1], in_dim_q=audio_output_shape[-1], out_dim=audio_output_shape[-1], num_heads=num_heads)\n",
    "        self.va1 = Attention(in_dim_k=audio_output_shape[-1], in_dim_q=video_output_shape[-1], out_dim=video_output_shape[-1], num_heads=num_heads)\n",
    "\n",
    "        self.final_mlp_layer_1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.final_mlp_layer_output = tf.keras.layers.Dense(8, activation='softmax', name=\"final_classification\")\n",
    "\n",
    "    def call(self,inputs, training=False):\n",
    "        x_audio, x_visual = inputs\n",
    "        x_audio = self.audio_stage1(x_audio)\n",
    "        x_visual = self.video_stage1(x_visual)\n",
    "\n",
    "        # proj_x_a = tf.transpose(x_audio, perm=[0,2,1])\n",
    "        # proj_x_v = tf.transpose(x_visual, perm=[0,2,1])\n",
    "\n",
    "        # _, h_av = self.av1((proj_x_v, proj_x_a))\n",
    "        # _, h_va = self.va1((proj_x_a, proj_x_v))\n",
    "\n",
    "        # if h_av.shape[1] > 1:  # if more than 1 head, take average\n",
    "        #     h_av = tf.reduce_mean(h_av, axis=1, keepdims=True)\n",
    "        # h_av = tf.reduce_sum(h_av, axis=-2)\n",
    "\n",
    "        # if h_va.shape[1] > 1:  # if more than 1 head, take average\n",
    "        #     h_va = tf.reduce_mean(h_va, axis=1, keepdims=True)\n",
    "        # h_va = tf.reduce_sum(h_va, axis=-2)\n",
    "\n",
    "        # x_audio = h_va * x_audio\n",
    "        # x_visual = h_av * x_visual\n",
    "        \n",
    "        # Passing through stage 2\n",
    "        x_audio = self.audio_stage2(x_audio)\n",
    "        x_visual = self.video_stage2(x_visual)\n",
    "        flat_audio = tf.keras.layers.Flatten()(x_audio)\n",
    "        flat_video = tf.keras.layers.Flatten()(x_visual)\n",
    "\n",
    "        combined = tf.keras.layers.concatenate([flat_audio, flat_video])\n",
    "        hidden = self.final_mlp_layer_1(combined)\n",
    "        sentiment = self.final_mlp_layer_output(hidden)\n",
    "        \n",
    "        return sentiment\n",
    "\n",
    "model = Model(video_input_shape=(100, 128, 128), audio_input_shape=(162,1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create dummy data\n",
    "dummy_audio_data = tf.random.normal([1, *model.audio_stage1.input_shape[1:]])\n",
    "dummy_video_data = tf.random.normal([1, *model.video_stage1.input_shape[1:]])\n",
    "\n",
    "# Pass the dummy data through the model\n",
    "_ = model([dummy_audio_data, dummy_video_data])\n",
    "\n",
    "# Now you can view the summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_dim_k, in_dim_q, out_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        head_dim = out_dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q = tf.keras.layers.Dense(out_dim, use_bias=qkv_bias)\n",
    "        self.kv = tf.keras.layers.Dense(out_dim * 2, use_bias=qkv_bias)\n",
    "        self.attn_drop = tf.keras.layers.Dropout(attn_drop)\n",
    "        self.proj = tf.keras.layers.Dense(out_dim)\n",
    "        self.proj_drop = tf.keras.layers.Dropout(proj_drop)\n",
    "        self.qkmatrix = None\n",
    "\n",
    "    def call(self, x, x_q):\n",
    "        B, Nk, Ck = tf.shape(x)\n",
    "        B, Nq, Cq = tf.shape(x_q)\n",
    "        \n",
    "        print(\"Shape of x:\", x.shape) # Debugging\n",
    "        print(\"Shape of x_q:\", x_q.shape) # Debugging\n",
    "\n",
    "        q = self.q(x_q)\n",
    "        q = tf.reshape(q, [B, Nq, 1, self.num_heads, -1])\n",
    "        q = tf.transpose(q, perm=[2, 0, 3, 1, 4])\n",
    "        \n",
    "        kv = self.kv(x)\n",
    "        kv = tf.reshape(kv, [B, Nk, 2, self.num_heads, -1])\n",
    "        kv = tf.transpose(kv, perm=[2, 0, 3, 1, 4])\n",
    "        k, v = tf.split(kv, 2, axis=0)\n",
    "        q = tf.squeeze(q, axis=0)\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        \n",
    "        self.qkmatrix = attn\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = tf.matmul(attn, v)\n",
    "        x = tf.transpose(x, perm=[1, 2, 0, 3])\n",
    "        x = tf.reshape(x, [B, Nq, -1])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x, self.qkmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(keras.layers.Layer):\n",
    "    def __init__(self, in_dim_k, in_dim_q, out_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        head_dim = out_dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q = keras.layers.Dense(out_dim, use_bias=qkv_bias)\n",
    "        self.kv = keras.layers.Dense(out_dim * 2, use_bias=qkv_bias)\n",
    "        self.attn_drop = keras.layers.Dropout(attn_drop)\n",
    "        self.proj = keras.layers.Dense(out_dim)\n",
    "        self.proj_drop = keras.layers.Dropout(proj_drop)\n",
    "        self.qkmatrix = None\n",
    "\n",
    "    def call(self, x, x_q):\n",
    "        B, Nk, Ck = tf.shape(x)\n",
    "        B, Nq, Cq = tf.shape(x_q)\n",
    "        \n",
    "        q = self.q(x_q)\n",
    "        q = tf.reshape(q, [B, Nq, 1, self.num_heads, -1])\n",
    "        q = tf.transpose(q, perm=[2, 0, 3, 1, 4])\n",
    "        \n",
    "        kv = self.kv(x)\n",
    "        kv = tf.reshape(kv, [B, Nk, 2, self.num_heads, -1])\n",
    "        kv = tf.transpose(kv, perm=[2, 0, 3, 1, 4])\n",
    "        k, v = tf.split(kv, 2, axis=0)\n",
    "        q = tf.squeeze(q, axis=0)\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        \n",
    "        self.qkmatrix = attn\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = tf.matmul(attn, v)\n",
    "        x = tf.transpose(x, perm=[1, 2, 0, 3])\n",
    "        x = tf.reshape(x, [B, Nq, -1])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x, self.qkmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_dim_k, in_dim_q, out_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = out_dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q = tf.keras.layers.Dense(out_dim, use_bias=qkv_bias)\n",
    "        self.kv = tf.keras.layers.Dense(out_dim * 2, use_bias=qkv_bias)\n",
    "        self.attn_drop = tf.keras.layers.Dropout(attn_drop)\n",
    "        self.proj = tf.keras.layers.Dense(out_dim)\n",
    "        self.proj_drop = tf.keras.layers.Dropout(proj_drop)\n",
    "        self.qkmatrix = None\n",
    "\n",
    "    def call(self, x, x_q):\n",
    "        B, Nk, Ck = x.shape\n",
    "        B, Nq, Cq = x_q.shape\n",
    "        q = tf.reshape(self.q(x_q), [B, Nq, 1, self.num_heads, -1])\n",
    "        q = tf.transpose(q, perm=[2, 0, 3, 1, 4])\n",
    "        kv = tf.reshape(self.kv(x), [B, Nk, 2, self.num_heads, -1])\n",
    "        kv = tf.transpose(kv, perm=[2, 0, 3, 1, 4])\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        q = tf.squeeze(q, axis=0)\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        self.qkmatrix = attn\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        current_shape = tf.shape(x_q)\n",
    "        B = current_shape[0]\n",
    "        Nq = current_shape[1]\n",
    "        q = tf.reshape(self.q(x_q), [B, Nq, 1, self.num_heads, -1])\n",
    "\n",
    "        x = tf.reshape(tf.transpose(tf.matmul(attn, v), perm=[1, 2, 0, 3]), [B, Nq, -1])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, self.qkmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "regularization_strength = 0.01\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_dim_k, in_dim_q, out_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        head_dim = out_dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # Define the layers\n",
    "        self.q = tf.keras.layers.Dense(out_dim, use_bias=qkv_bias)\n",
    "        self.kv = tf.keras.layers.Dense(out_dim * 2, use_bias=qkv_bias)\n",
    "        self.attn_drop = tf.keras.layers.Dropout(attn_drop)\n",
    "        self.proj = tf.keras.layers.Dense(out_dim)\n",
    "        self.proj_drop = tf.keras.layers.Dropout(proj_drop)\n",
    "        self.qkmatrix = None\n",
    "        # self.post_attention_pooling = tf.keras.layers.MaxPooling1D(pool_size=2) \n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, x_q = inputs\n",
    "        _, Nk, Ck = x.shape\n",
    "        _, Nq, Cq = x_q.shape\n",
    "\n",
    "        q = self.q(x_q)\n",
    "        q = tf.reshape(q, [-1, Nq, self.num_heads, tf.shape(q)[-1]])\n",
    "        q = tf.transpose(q, [0, 2, 1, 3])\n",
    "\n",
    "        kv = self.kv(x)\n",
    "        kv = tf.reshape(kv, [-1, Nk, 2, self.num_heads, tf.shape(kv)[-1] // 2])\n",
    "        kv = tf.transpose(kv, [2, 0, 3, 1, 4])\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        \n",
    "        self.qkmatrix = attn\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = tf.matmul(attn, v)\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        x = tf.reshape(x, [-1, Nq, self.num_heads * tf.shape(x)[-1]])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        # x = self.post_attention_pooling(x)\n",
    "\n",
    "        return x, self.qkmatrix\n",
    "\n",
    "\n",
    "# Video stages\n",
    "def video_stage1(input_shape):\n",
    "    video_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu')(video_input)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    shape = x.get_shape().as_list()\n",
    "    x = tf.keras.layers.Reshape((shape[1]*shape[2], shape[3]))(x)\n",
    "    x = tf.keras.layers.Conv1D(128, 3, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Conv1D(64, 3, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    return tf.keras.Model(inputs=video_input, outputs=x, name=\"video_stage1_model\")\n",
    "\n",
    "def video_stage2(input_shape_stage2):\n",
    "    # Stage 2\n",
    "    vedio_input_stage2 = tf.keras.layers.Input(shape=input_shape_stage2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 3, activation='relu')(vedio_input_stage2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(pool_size=2)(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(32, 3, activation='relu')(x2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(pool_size=2)(x2)\n",
    "    x2 = tf.keras.layers.Dropout(0.3)(x2)\n",
    "    return tf.keras.Model(inputs=vedio_input_stage2, outputs=x2, name=\"video_stage2_model\")\n",
    "\n",
    "def audio_stage1(input_shape):\n",
    "    audio_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    # x = tf.keras.layers.Reshape((-1, 1))(audio_input)\n",
    "    x = tf.keras.layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(audio_input)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    return tf.keras.Model(inputs=audio_input, outputs=x, name=\"audio_stage1_model\")\n",
    "\n",
    "def audio_stage2(input_shape_stage2):\n",
    "    audio_input_stage2 = tf.keras.layers.Input(shape=input_shape_stage2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu')(audio_input_stage2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(32, kernel_size=5, strides=1, padding='same', activation='relu')(x2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dropout(0.3)(x2)\n",
    "    return tf.keras.Model(inputs=audio_input_stage2, outputs=x2, name=\"audio_stage2_model\")\n",
    "\n",
    "\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, video_input_shape, audio_input_shape, num_heads=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.video_stage1 = video_stage1(video_input_shape)\n",
    "        self.audio_stage1 = audio_stage1(audio_input_shape)\n",
    "\n",
    "\n",
    "\n",
    "        # Getting the shapes after stage 1 transformations\n",
    "        video_output_shape = self.video_stage1.layers[-1].output_shape[1:]\n",
    "        audio_output_shape = self.audio_stage1.layers[-1].output_shape[1:]\n",
    "\n",
    "\n",
    "        audio_output_shape_after_attention = (audio_output_shape[0], 64)\n",
    "        vedio_output_shape_after_attention = (video_output_shape[0], 64)\n",
    "        # audio_output_shape_after_attention = (video_output_shape[-1])\n",
    "        # vedio_output_shape_after_attention = (audio_output_shape[-1])\n",
    "\n",
    "        self.audio_stage2 = audio_stage2(audio_output_shape_after_attention)\n",
    "        self.video_stage2 = video_stage2(vedio_output_shape_after_attention)\n",
    "\n",
    "        self.av1 = Attention(in_dim_k=video_output_shape[-1], in_dim_q=audio_output_shape[-1], out_dim=audio_output_shape[-1], num_heads=num_heads)\n",
    "        self.va1 = Attention(in_dim_k=audio_output_shape[-1], in_dim_q=video_output_shape[-1], out_dim=video_output_shape[-1], num_heads=num_heads)\n",
    "\n",
    "        self.final_mlp_layer_1 = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization_strength))\n",
    "        self.final_mlp_layer_output = tf.keras.layers.Dense(8, activation='softmax', name=\"final_classification\")\n",
    "\n",
    "    def call(self,inputs, training=False):\n",
    "        x_audio, x_visual = inputs\n",
    "        x_audio = self.audio_stage1(x_audio)\n",
    "        x_visual = self.video_stage1(x_visual)\n",
    "\n",
    "        proj_x_a = tf.transpose(x_audio, perm=[0,2,1])\n",
    "        proj_x_v = tf.transpose(x_visual, perm=[0,2,1])\n",
    "\n",
    "        _, h_av = self.av1((proj_x_v, proj_x_a))\n",
    "        _, h_va = self.va1((proj_x_a, proj_x_v))\n",
    "\n",
    "        if h_av.shape[1] > 1:  # if more than 1 head, take average\n",
    "            h_av = tf.reduce_mean(h_av, axis=1, keepdims=True)\n",
    "\n",
    "        if h_va.shape[1] > 1:  # if more than 1 head, take average\n",
    "            h_va = tf.reduce_mean(h_va, axis=1, keepdims=True)\n",
    "\n",
    "        h_av = tf.reduce_sum(h_av, axis=-2)\n",
    "        h_va = tf.reduce_sum(h_va, axis=-2)\n",
    "\n",
    "        x_audio = h_va * x_audio\n",
    "        x_visual = h_av * x_visual\n",
    "\n",
    "        # h_av = tf.squeeze(tf.reduce_mean(h_av, axis=-2, keepdims=True), axis=1)\n",
    "        # h_va = tf.squeeze(tf.reduce_mean(h_va, axis=-2, keepdims=True), axis=1)\n",
    "\n",
    "\n",
    "        # x_audio = tf.multiply(h_va, x_audio)\n",
    "        # x_visual = tf.multiply(h_av, x_visual)\n",
    "\n",
    "        # print(\"Expected audio reshape:\", self.audio_stage2.input_shape[1:])\n",
    "        # print(\"Expected visual reshape:\", self.video_stage2.input_shape[1:])\n",
    "\n",
    "        # x_audio, _ = self.av1((proj_x_v, proj_x_a))\n",
    "        # print(\"Shape after audio_stage1:\", x_audio.shape)\n",
    "        # x_visual, _ = self.va1((proj_x_a, proj_x_v))\n",
    "        # print(\"Shape after attention:\", x_audio.shape)\n",
    "\n",
    "        # # Expected shape\n",
    "        # expected_shape_audio = (x_audio.shape[1], x_audio.shape[2])\n",
    "        # expected_shape_video = (x_visual.shape[1], x_visual.shape[2])\n",
    "\n",
    "        # x_audio = tf.reshape(x_audio, [-1, *expected_shape_audio])\n",
    "        # x_visual = tf.reshape(x_visual, [-1, *expected_shape_video])\n",
    "\n",
    "        # print(\"shape after reshape audio: \", x_audio.shape)\n",
    "        # print(\"shape after reshape vedio: \", x_visual.shape)\n",
    "        \n",
    "        # Passing through stage 2\n",
    "        x_audio = self.audio_stage2(x_audio)\n",
    "        x_visual = self.video_stage2(x_visual)\n",
    "        flat_audio = tf.keras.layers.Flatten()(x_audio)\n",
    "        flat_video = tf.keras.layers.Flatten()(x_visual)\n",
    "\n",
    "        combined = tf.keras.layers.concatenate([flat_audio, flat_video])\n",
    "        hidden = self.final_mlp_layer_1(combined)\n",
    "        hidden = tf.keras.layers.Dropout(0.3)(hidden)\n",
    "        sentiment = self.final_mlp_layer_output(hidden)\n",
    "        \n",
    "        return sentiment\n",
    "\n",
    "model = Model(video_input_shape=(100, 128, 128), audio_input_shape=(162,1))\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create dummy data\n",
    "dummy_audio_data = tf.random.normal([1, *model.audio_stage1.input_shape[1:]])\n",
    "dummy_video_data = tf.random.normal([1, *model.video_stage1.input_shape[1:]])\n",
    "\n",
    "# Pass the dummy data through the model\n",
    "_ = model([dummy_audio_data, dummy_video_data])\n",
    "\n",
    "# Now you can view the summary\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3437ef53a2a94a5a8dae6ac06b952d2bb20caec6a287552291b295f1ffc2666a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
